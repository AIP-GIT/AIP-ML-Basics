{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_Covid_Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp-FQcAgwE9o"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMnf97tELmEu",
        "outputId": "13952711-a761-4daf-9607-ea5795831ab2"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKB9tbOLwXwe",
        "outputId": "f0eb5cdb-37a1-46aa-f8ed-bfd6f070ab84"
      },
      "source": [
        "import re\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.corpus import twitter_samples \r\n",
        "\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords \r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73HpVDmdL1_E"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJlt1fXEMCuh"
      },
      "source": [
        "test_pos = all_positive_tweets[4000:]\r\n",
        "train_pos = all_positive_tweets[:4000]\r\n",
        "test_neg = all_negative_tweets[4000:]\r\n",
        "train_neg = all_negative_tweets[:4000]\r\n",
        "\r\n",
        "train_x = train_pos + train_neg \r\n",
        "test_x = test_pos + test_neg\r\n",
        "\r\n",
        "# combine positive and negative labels\r\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\r\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbF4YCFJMFI7"
      },
      "source": [
        "# Function to tokenize and preprocess the data\r\n",
        "def preprocess_data(line):\r\n",
        "    line = re.sub(r'#','',line)\r\n",
        "    line = re.sub(r'@','',line)\r\n",
        "    clean_line = [word for word in word_tokenize(line) if word not in string.punctuation and word not in stop_words]\r\n",
        "    clean_line= \"\".join(clean_line)\r\n",
        "    return clean_line"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw_QYP5fMlGy",
        "outputId": "131e081c-38a4-4896-830f-fab28db2622b"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlggiAq9MOsZ"
      },
      "source": [
        "train_x = [preprocess_data(line) for line in train_x]\r\n",
        "test_x = [preprocess_data(line) for line in test_x]\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTgg1_3MvSk"
      },
      "source": [
        "# Try tf-idf and logistic regression\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english',max_features=128)\r\n",
        "docs_tfidf=tfidf_vectorizer.fit_transform(train_x)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y2GOya6Su-T",
        "outputId": "24af07c1-090e-4c96-a539-aaa87f16263b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "clf=LogisticRegression()\r\n",
        "clf.fit(docs_tfidf,train_y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ77z8UqaJDI",
        "outputId": "3a021889-f9b5-464a-e17d-27da12e60ea3"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "lin_reg_pred=clf.predict(docs_tfidf)\r\n",
        "lin_reg_mse = mean_squared_error(train_y,lin_reg_pred)\r\n",
        "lin_reg_mse"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EjVAdGZpOem"
      },
      "source": [
        "**underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqc5xyFqogh2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english',max_features=128)\r\n",
        "docs_tfidf=tfidf_vectorizer.fit_transform(test_x)\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp7Y92yqkw_D",
        "outputId": "d8c77f1f-9017-4896-f266-46e324203c94"
      },
      "source": [
        "lin_reg_pred=clf.predict(docs_tfidf)\r\n",
        "lin_reg_mse = mean_squared_error(test_y,lin_reg_pred)\r\n",
        "lin_reg_mse"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "2PE24i4Nq58G",
        "outputId": "e3e5614f-d15d-4d70-ec9a-0a5e4a52c202"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "pred_train=clf.predict(docs_tfidf)\r\n",
        "pred_train_data=pred_train.reshape((288,1))\r\n",
        "print('accuracy_score on training data: ',accuracy_score(train_y,pred_train_data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c5aee864d145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpred_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_train_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy_score on training data: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2000 into shape (288,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfb13OE5x6ek"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}